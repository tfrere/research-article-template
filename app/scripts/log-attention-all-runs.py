#!/usr/bin/env python3
"""
Script pour logger tous les runs d'attention_loss.csv dans un seul projet
Comparaison des diffÃ©rentes architectures d'attention
"""

import trackio
import pandas as pd
import sys
from pathlib import Path

def log_attention_loss_all_runs():
    """Log attention_loss avec tous les runs dans le mÃªme projet"""
    print("ğŸš€ Logging attention_loss avec TOUS les runs dans le mÃªme projet...")
    
    # Chemin vers le fichier de donnÃ©es
    data_file = Path("src/content/assets/data/attention_loss.csv")
    
    if not data_file.exists():
        print(f"âŒ Fichier non trouvÃ©: {data_file}")
        return False
    
    try:
        # Charger les donnÃ©es
        df = pd.read_csv(data_file)
        print(f"ğŸ“ DonnÃ©es chargÃ©es: {len(df)} lignes")
        
        # Obtenir les runs uniques
        runs = df['run_name'].unique()
        print(f"ğŸ” Runs Ã  crÃ©er dans le mÃªme projet ({len(runs)}):")
        for run in runs:
            count = len(df[df['run_name'] == run])
            print(f'  - "{run}": {count} points')
        
        # Logger chaque run dans le MÃŠME projet
        for i, run_name in enumerate(runs):
            print(f"\nğŸŒ CrÃ©ation du run: \"{run_name}\"")
            
            # Utiliser le mÃªme projet pour tous les runs
            project_name = "attention-loss-comparison"
            
            # Initialiser Trackio avec le mÃªme projet
            trackio.init(
                project=project_name, 
                space_id="tfrere/loss-experiment",
                name=run_name,
                resume="allow"  # Permettre de reprendre ou crÃ©er un nouveau run
            )
            
            # Filtrer les donnÃ©es pour ce run
            run_data = df[df['run_name'] == run_name]
            print(f"ğŸ“Š Logging de {len(run_data)} points pour \"{run_name}\"...")
            print(f"ğŸ”§ Projet utilisÃ©: {project_name}")
            
            # Logger les donnÃ©es de ce run
            for j, (_, row) in enumerate(run_data.iterrows()):
                trackio.log({
                    "tokens": float(row['tokens']),
                    "loss": float(row['loss']),
                    "step": j
                })
                
                if j % 100 == 0:
                    print(f"  âœ… Ã‰tape {j}: tokens={row['tokens']:.0f}, loss={row['loss']:.4f}")
            
            # Finaliser ce run
            trackio.finish()
            print(f"âœ… Run \"{run_name}\" terminÃ©!")
        
        print(f"\nğŸ‰ Tous les runs d'attention crÃ©Ã©s dans le mÃªme projet!")
        print(f"ğŸ“Š {len(runs)} runs loggÃ©s dans le projet: {project_name}")
        return True
        
    except Exception as e:
        print(f"âŒ Erreur lors du logging: {e}")
        return False

def main():
    print("ğŸ¯ CrÃ©ation de TOUS les runs d'attention dans le MÃŠME projet")
    print("=" * 60)
    print("ğŸ”„ Comparaison des architectures d'attention")
    print("=" * 60)
    
    # Logger avec le mÃªme projet
    success = log_attention_loss_all_runs()
    
    print("\nğŸ“Š RÃ©sultat:")
    if success:
        print("âœ… Tous les runs d'attention crÃ©Ã©s dans le mÃªme projet!")
        print("ğŸ“Š Consultez votre dashboard Trackio: https://huggingface.co/spaces/tfrere/loss-experiment")
        print("ğŸ” Vous devriez voir 1 projet avec 6 runs:")
        print('   - "GQA 8 groups (baseline)"')
        print('   - "MHA"')
        print('   - "GQA 4 groups"')
        print('   - "MQA"')
        print('   - "GQA 2 groups"')
        print('   - "GQA 16 groups"')
        print("ğŸ“ˆ Les 6 courbes devraient Ãªtre dans le mÃªme graphique!")
        print("ğŸ”¬ Comparaison des architectures d'attention: GQA vs MHA vs MQA")
    else:
        print("âŒ Ã‰chec du logging")
    
    return 0 if success else 1

if __name__ == "__main__":
    exit(main())
