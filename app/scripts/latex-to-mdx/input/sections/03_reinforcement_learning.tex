\section{Robot (Reinforcement) Learning}
\label{sec:learning-rl}

\epigraph{\textit{Approximate the solution, not the problem} [...]}{Richard Sutton}

\begin{tldr}
The need for expensive high-fidelity simulators can be obviated by learning from real-world data, using sample-efficient algorithms that can safely train directly on hardware.
\end{tldr}

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/ch3/ch3-learning-benefits.png}
    \caption{Learning-based robotics streamlines perception-to-action by learning a (1) unified high-level controller capable to take (2) high-dimensional, unstructured sensorimotor information. Learning (3) does not require a dynamics model and instead focuses on interaction data, and (4) empirically correlates with
    the scale of the data used.
    }
    \label{fig:robot-learning-upsides}
\end{figure}

Learning-based techniques for robotics naturally address the limitations presented in~\ref{sec:classical} (Figure~\ref{fig:robot-learning-upsides}).
Learning-based techniques typically rely on prediction-to-action (\emph{visuomotor policies}), thereby directly mapping sensorimotor inputs to predicted actions, streamlining control policies by removing the need to interface multiple components.
Mapping sensorimotor inputs to actions directly also allows to add diverse input modalities, leveraging the automatic feature extraction characteristic of most modern learning systems.
Further, learning-based approaches can in principle entirely bypass modeling efforts and instead rely exclusively on interactions data, proving transformative when dynamics are challenging to model or even entirely unknown.
Lastly, learning for robotics (\emph{robot learning}) is naturally well posed to leverage the growing amount of robotics data openly available, just as computer vision first and natural language processing later did historically benefit from large scale corpora of (possibly non curated) data, in great part overlooked by dynamics-based approaches.

Being a field at its relative nascent stages, no prevalent technique(s) proved distinctly better better in robot learning.
Still, two major classes of methods gained prominence: \highlight{reinforcement learning (RL)} and \highlight{Behavioral Cloning (BC)} (Figure~\ref{fig:robot-learning-atlas}).
In this section, we provide a conceptual overview of applications of the former to robotics, as well as introduce practical examples of how to use RL within \lerobot.
We then introduce the major limitations RL suffers from, to introduce BC techniques in the next sections (\ref{sec:learning-bc-single, sec:learning-bc-generalist}).

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{figures/ch3/ch3-learning-atlas.png}
    \caption{Overview of the robot learning methods implemented in \lerobot.}
    \label{fig:robot-learning-atlas}
\end{figure}

In Figure~\ref{fig:robot-learning-atlas} we decided to include generalist robot models~\citep{black$p_0$VisionLanguageActionFlow2024,shukorSmolVLAVisionLanguageActionModel2025} alongside task-specific BC methods.
While significant different in spirit---\emph{generalist} models are language-conditioned and use instructions to generate motion valid across many tasks, while \emph{task-specific} models are typically not language-conditioned and used to perform a single task---foundation models are largely trained to reproduce trajectories contained in a large training set of input demonstrations.
Thus, we argue generalist policies can indeed be grouped alongside other task-specific BC methods, as they both leverage similar training data and schemas.

Figure~\ref{fig:robot-learning-atlas} illustrates this categorization graphically, explicitly listing all the robot learning policies currently available in \lerobot: Action Chunking with Transformers (ACT)~\citep{zhaoLearningFineGrainedBimanual2023}, Diffusion Policy~\citep{chiDiffusionPolicyVisuomotor2024}, Vector-Quantized Behavior Transformer (VQ-BeT)~\citep{leeBehaviorGenerationLatent2024}, \( \pi_0 \)~\citep{black$p_0$VisionLanguageActionFlow2024}, SmolVLA~\citep{shukorSmolVLAVisionLanguageActionModel2025}, Human-in-the-loop Sample-efficient RL (HIL-SERL)~\citep{luoPreciseDexterousRobotic2024} and TD-MPC~\citep{hansenTemporalDifferenceLearning2022}.


\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{figures/ch3/ch3-rl-examples.png}
    \caption{Examples of two different robotics tasks performed using RL. In the manipulation task (A) an agent learns to reach for a yellow plastic block in its environment, and to put it inside of a box. In the locomotion task (B) an agent learns to move its center of mass sideways without falling.}
    \label{fig:robotics-with-rl-examples}
\end{figure}

Applications of RL to robotics have been long studied, to the point the relationship between these two disciplines has been compared to that between physics and matematics~\citep{koberReinforcementLearningRobotics}.
Indeed, due to their interactive and sequential nature, many robotics problems can be directly mapped to RL problems.
Figure~\ref{fig:robotics-with-rl-examples} depicts two of such cases. 
Reaching for an object to move somewhere else in the scene is an indeed sequential problem where at each cycle the controller needs to adjust the position of the robotic arm based on their current configuration and the (possibly varying) position of the object.
Figure~\ref{fig:robotics-with-rl-examples} also shows an example of a locomotion problem, where sequentiality is inherent in the problem formulation. 
While sliding to the side, the controller has to constantly keep adjusting to the robot's propioperception to avoid failure (falling).

\subsection{A (Concise) Introduction to RL}
The RL framework~\citep{suttonReinforcementLearningIntroduction2018}, which we briefly introduce here, has often been used to model robotics problems~\citep{koberReinforcementLearningRobotics}.
RL is a subfield within ML fundamentally concerned with the development of autonomous systems (\emph{agents}) learning how to \emph{continuously behave} in an evolving environment, developing (ideally, well-performing) control strategies (\emph{policies}).
Crucially for robotics, RL agents can improve via trial-and-error only, thus entirely bypassing the need to develop explicit models of the problem dynamics, and rather exploiting interaction data only.
In RL, this feedback loop (Figure~\ref{fig:rl-most-famous-pic}) between actions and outcomes is established through the agent sensing a scalar quantity (\emph{reward}).

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{figures/ch3/ch3-agent-env.png}
    \caption{Agent-Environment interaction diagram (image credits to~\citet{suttonReinforcementLearningIntroduction2018}).}
    \label{fig:rl-most-famous-pic}
\end{figure}

Formally, interactions between an agent and its environment are typically modeled via a Markov Decision Process (MDP)~\citep{bellmanMarkovianDecisionProcess1957}.
Representing robotics problems via MDPs offers several advantages, including (1) incorporating uncertainty through MDP's inherently stochastic formulation and (2) providing a theoretically sound framework for learning \emph{without} an explicit dynamic model.
While accommodating also a continuous time formulation, MDPs are typically considered in discrete time in RL, thus assuming interactions to atomically take place over the course of discrete \emph{timestep} \( t=0,1,2,3, \dots, T \).
MDPs allowing for an unbounded number of interactions ( \( T \to + \infty \) ) are typically termed \emph{infinite-horizon}, and opposed to \emph{finite-horizon} MDPs in which \( T \) cannot grow unbounded.
Unless diversely specified, we will only be referring to discrete-time finite-horizon (\emph{episodic}) MDPs here.

Formally, a lenght-\(T\) Markov Decision Process (MDP) is a tuple \( \mathcal M = \langle \statespace, \actionspace, \dynamics, r, \gamma, \rho, T \rangle \), where:
\begin{itemize}
    \item \(\statespace\) is the \emph{state space}; \(\state \in \statespace\) denotes the (possibly non-directly observable) environment state at time \(t\). In robotics, states often comprise robot configuration and velocities (\(q_t, \dot q_t\)), and can accomodate sensor readings such as camera or audio streams.
    \item \(\actionspace\) is the \emph{action space}; \(\action \in \actionspace\) may represent joint torques, joint velocities, or even end-effector commands. In general, actions correspond to commands intervenings on the configuration of the robot. 
    \item \(\dynamics\) represents the (possibly non-deterministic) environment dynamics, with \(\dynamics: \statespace \times \actionspace \times \statespace \mapsto [0, 1] \) corresponding to \( \dynamics \, \transition = \transitionprob \). For instance, for a planar manipulator dynamics could be considered deterministic when the environment is fully described (Figure~\ref{fig:planar-manipulation-simple}), and stochastic when unmodeled disturbances depending on non-observable parameters intervene (Figure~\ref{fig:planar-manipulator-box-velocity}).
    \item \(r: \statespace \times \actionspace \times \statespace \to \mathbb R\) is the \emph{reward function}, weighing the transition \( \transition \) in the context of the achievement of an arbitrary goal. For instance, a simple reward function for quickly moving the along the \( x \) axis in 3D-space (Figure~\ref{fig:robotics-with-rl-examples}) could be based on the absolute position of the robot along the \( x \) axis~(\(p_x\)), present negative penalties for falling over (measured from \( p_z \)) and a introduce bonuses \( \dot p_x \) for speed, \(r \transition \equiv r(\state) = p_{x_t} \cdot \dot p_{x_t} - \tfrac{1}{p_{z_t}} \).
\end{itemize}
Lastly, \(\gamma \in [0,1] \) represent the discount factor regulating preference for immediate versus long-term reward (with an effective horizon equal to \( \tfrac{1}{1-\gamma} \)), and \( \rho \) is the distribution, defined over \(\statespace \), the MDP's \emph{initial} state is sampled from, \( s_0 \sim \rho \).

A length-\(T\) \emph{trajectory} is the (random) sequence
\begin{equation}\label{eq:trajectory_definition}
    \tau = \trajectory,
\end{equation}
with per-step rewards defined as \(r_t = r \transition \) for ease of notation.Interestingly, assuming both the environment dynamics and conditional distribution over actions given states---the \emph{policy}---to be \emph{Markovian}:
%
\begin{align}
\mathbb P(\stateplusone \vert s_t, a_t, s_{t-1}, a_{t-1}, \dots s_0, a_0 ) &= \mathbb P \transitiongiven \label{eq:dynamics_markovian} \\
\mathbb P(\action \vert \state, a_{t-1}, s_{t-1}, s_0, a_0) &= \mathbb P(\action \vert \state) \label{eq:policy_markovian}
\end{align}
%
The probability of observing a given trajectory \( \tau \) factorizes into
\begin{equation}\label{eq:traj_prob}
    \mathbb P(\tau) = \mathbb P (s_0) \prod_{t=0}^{T-1} \mathbb P \transitiongiven \ \mathbb P(\action \vert \state).
\end{equation}

Policies \( \mathbb P(\action \vert \state) \) are typically indicated as \( \pi(\action \vert \state) \), and often parametrized via \( \theta \), yielding \( \pi_\theta (\action \vert \state )\).
Policies are trained optimizing the (discounted) \emph{return} associated to a given \( \tau \), i.e. the (random) sum of measured rewards over trajectory:
\[
    G(\tau) = \sum_{t=0}^{T-1} \gamma^{t} r_t.
\]
In that, agents seek to learn control strategies (\emph{policies}, \( \pi_\theta \)) maximizing the expected return \( \mathbb E_{\tau \sim \pi_\theta} G(\tau) \). 
For a given dynamics \( \mathcal D \)---i.e., for a given problem---taking the expectation over the (possibly random) trajectories resulting from acting according to a certain policy provides a direct, goal-conditioned ordering in the space of all the possible policies \( \Pi \), yielding the (maximization) target \( J : \Pi \mapsto \mathbb R \)
\begin{align}
    J(\pi_\theta) &= \mathbb E_{\tau \sim \mathbb P_{\theta; \mathcal D}} \left[ G(\tau) \right], \label{eq:RL-j-function} \\
    \mathbb P_{\theta; \mathcal D} (\tau) &= \rho \prod_{t=0}^{T-1} \mathcal D \transition \ \pi_\theta (\action \vert \state).\label{eq:traj-probabilities-for-policies}
\end{align}

Because in the RL framework the agent is assumed to only be able to observe the environment dynamics and not to intervene on them,~\ref{eq:RL-j-function} varies exclusively with the policy followed.
In turn, MDPs naturally provide a framework to optimize over the space of the possible behaviors an agent might enact (\( \pi \in \Pi \)), searching for the \emph{optimal policy} \( \pi^* = \arg \max_{\theta} J(\pi_\theta) \), where \( \theta \) is the parametrization adopted by the policy set \( \Pi: \pi_\theta \in \Pi, \ \forall \theta \).
Other than providing a target for policy search, \( G(\tau) \) can also be used as a target to discriminate between states and state-action pairs.
Given any state \( s \in \statespace \)---e.g., a given configuration of the robot---the \emph{state-value} function
\[
    V_\pi(s) = \mathbb E_{\tau \sim \pi} \left[ G(\tau) \big \vert s_0 = s \right]
\]
can be used to discriminate between desirable and undesirable state in terms of long-term (discounted) reward maximization, under a given policy \(\pi\).
Similarily, the \emph{state-action} value function also conditions the cumulative discounted reward on selecting action \( a \) when in \( s \), and thereafter act according to \( \pi \):
\[
    Q_\pi(s,a) = \mathbb E_{\tau \sim \pi} \left[ G (\tau) \big \vert s_0 = s, a_0=a \right]
\]
Crucially, value functions are interrelated:
\begin{align}
Q_\pi(s_t, a_t) &= \mathbb{E}_{\stateplusone \sim \mathbb P(\bullet \vert \state, \action)} \left[ r_t + \gamma V_\pi(\stateplusone) \right] \label{eq:q-as-v} \\
V_\pi(\state) &= \mathbb E_{\action \sim \pi(\bullet \vert \state)} \left[ Q_\pi (\state, \action) \right]
\label{eq:v-as-q}
\end{align}
Inducing an ordering over states and state-action pairs under \( \pi \), value functions are central to most RL algorithms.
A variety of methods have been developed in RL as standalone attemps to find (approximate) solutions to the problem of maximizing cumulative reward (Figure~\ref{fig:rl-algos-atlas}).

\begin{figure}
    \centering
    \includegraphics[width=0.4\textwidth]{figures/ch3/ch3-rl-algorithms-atlas.png}
    \caption{Popular RL algorithms. See~\citet{SpinningUp2018} for a complete list of citations.}
    \label{fig:rl-algos-atlas}
\end{figure}

Popular approaches to continuous state and action space---such as those studied within robotics---include~\citet{schulmanTrustRegionPolicy2017, schulmanProximalPolicyOptimization2017, haarnojaSoftActorCriticOffPolicy2018}.
Across manipulation~\citep{akkayaSolvingRubiksCube2019} and locomotion~\citep{leeLearningQuadrupedalLocomotion2020} problems, RL proved extremely effective in providing a platform to (1) adopt a unified, streamlined perception-to-action pipeline, (2) natively integrate propioperception with multi-modal high-dimensional sensor streams  (3) disregard a description of the environment dynamics, by focusing on observed interaction data rather than modeling, and (4) anchor policies in the experience collected and stored in datasets.
For a more complete survey of applications of RL to robotics, we refer the reader to~\citet{koberReinforcementLearningRobotics,tangDeepReinforcementLearning2024}.

\subsection{Real-world RL for Robotics}
Streamlined end-to-end control pipelines, data-driven feature extraction and a disregard for explicit modeling in favor of interaction data are all features of RL for robotics.
However, particularly in the context of real-world robotics, RL still suffers from limitations concerning machine safety and learning efficiency.

First, especially early in training, \highlight{actions are typically explorative, and thus erractic}.
On physical systems, untrained policies may command high velocities, self-collisiding configurations, or torques exceeding joint limits, leading to wear and potential hardware damage.
Mitigating these risks requires external safeguards (e.g., watchdogs, safety monitors, emergency stops), often incuring in a high degree of human supervision.
Further, in the typical episodic setting considered in most robotics problems, experimentation is substantially slowed down by the need to manually reset the environment over the course of training, a time-consuming and brittle process.

Second, learning with a limited number of samples remains problematic in RL, \highlight{limiting the applicability of RL in real-world robotics due to consequently prohibitive timescales of training}.
Even strong algorithms such as SAC~\citep{haarnojaSoftActorCriticOffPolicy2018} typically require a large numbers of transitions \( \{ \sars \}_{t=1}^N \).
On hardware, generating these data is time-consuming and can even be prohibitive.

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{figures/ch3/ch3-duck-sim-vs-real.png}
    \caption{Simulated (left) vs. real-world (right) OpenDuck. Discrepancies in the simulation dynamics (\emph{reality gap}) pose risks to policy transfer.}
    \label{fig:synthetic-vs-real-duck}
\end{figure}

Training RL policies in simulation~\citep{tobinDomainRandomizationTransferring2017} addresses both issues: it eliminates physical risk and dramatically increases throughput.
Yet, simulators require significant modeling effort, and rely on assumptions (simplified physical modeling, instantaneous actuation, static environmental conditions, etc.) limiting transferring policies learned in simulation due the discrepancy between real and simulated environments (\emph{reality gap}, Figure~\ref{fig:synthetic-vs-real-duck}).
\emph{Domain randomization} (DR) is a popular technique to overcome the reality gap, consisting in randomizing parameters of the simulated environment during training, to induce robustness to specific disturbances.
In turn, DR is employed to increase the diversity of scenarios over the course of training, improving on the chances sim-to-real transfer~\citep{akkayaSolvingRubiksCube2019,antonovaReinforcementLearningPivoting2017,jiDribbleBotDynamicLegged2023}.
In practice, DR is performed further parametrizing the \emph{simulator}'s dynamics \( \mathcal D \equiv \mathcal D_\xi \) with a \emph{dynamics} (random) vector \( \xi \) drawn an arbitrary distribution, \( \xi \sim \Xi \).
Over the course of training---typically at each episode's reset---a new \( \xi \) is drawn, and used to specify the environment's dynamics for that episode.
For instance, one could decide to randomize the friction coefficient of the surface in a locomotion task (Figure~\ref{fig:ducks-on-terrains}), or the center of mass of an object for a manipulation task.

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/ch3/ch3-many-ducks.png}
    \caption{The same locomotion task can be carried out in different (simulated) domains (exemplified by the difference in terrains) at training time, resulting to increased robustness over diverse environment dynamics.}
    \label{fig:ducks-on-terrains}
\end{figure}

While effective in transfering policies across the reality gap in real-world robotics~\citep{tobinDomainRandomizationTransferring2017,akkayaSolvingRubiksCube2019, jiDribbleBotDynamicLegged2023,tiboniDomainRandomizationEntropy2024}, DR often requires extensive manual engineering.
First, identifying which parameters to randomize---i.e., the \emph{support} \( \text{supp} (\Xi) \) of \( \Xi \)---is an inherently task specific process.
When locomoting over different terrains, choosing to randomize the friction coefficient is a reasonable choice, yet not completely resolutive as other factors (lightning conditions, external temperature, joints' fatigue, etc.) may prove just as important, making selecting these parameters yet another source of brittlness.

Selecting the dynamics distribution \( \Xi \) is also non-trivial.
On the one hand, distributions with low entropy might risk to cause failure at transfer time, due to the limited robustness induced over the course of training.
On the other hand, excessive randomization may cause over-regularization and hinder performance.
Consequently, the research community investigated approaches to automatically select the randomization distribution \( \Xi \), using signals from the training process or tuning it to reproduce observed real-world trajectories.
~\citet{akkayaSolvingRubiksCube2019} use a parametric uniform distribution \( \mathcal U(a, b) \) as \( \Xi \), widening the bounds as training progresses and the agent's performance improves (AutoDR).
While effective, AutoDR requires significant tuning---the bounds are widened by a fixed, pre-specified amount \( \Delta \)---and may disregard data when performance \emph{does not} improve after a distribution update~\citep{tiboniDomainRandomizationEntropy2024}.
~\citet{tiboniDomainRandomizationEntropy2024} propose a similar method to AutoDR (DORAEMON) to evolve \( \Xi \) based on training signal, but with the key difference of explicitly maximizing the entropy of parametric Beta distributions, inherently more flexible than uniform distributions.
DORAEMON proves particularly effective at dynamically increasing the entropy levels of the training distribution by employing a max-entropy objective, under performance constraints formulation.
Other approaches to automatic DR consist in specifically tuning \( \Xi \) to align as much as possible the simulation and real-world domains.
For instance, ~\citet{chebotar2019closing} interleave in-simulation policy training with repeated real-world policy rollouts used to adjust \( \Xi \) based on real-world data, while ~\citet{tiboniDROPOSimtoRealTransfer2023} leverage a single, pre-collected set of real-world trajectories and tune \( \Xi \) under a simple likelihood objective.

While DR has shown promise, it does not address the main limitation that, even under the assumption that an ideal distribution \( \Xi \) to sample from was indeed available, many robotics problems \highlight{cannot be simulated with high-enough fidelity under practical computational constraints} in the first place.
Simulating contact-rich manipulation of possibly deformable or soft materials---i.e., \emph{folding a piece of clothing}---can be costly and even time-intensive, limiting the benefits of in-simulation training.

A perhaps more foundamental limitation of RL for robotics is the general unavailability of complicated tasks' \emph{dense} reward function, the design of which is essentially based on human expertise and trial-and-error.
In practice, \emph{sparse} reward functions can be used to conclude whether one specific goal has been attained---\emph{has this t-shirt been correctly folded?}---but unfortunately incur in more challenging learning.
As a result, despite notable successes, deploying RL directly on real-world robots at scale remains challenging.

To make the most of (1) the growing number of openly available datasets and (2) relatively inexpensive robots like the SO-100, RL could (1) be anchored in already-collected trajectories---limiting erratic and dangerous exploration---and (2) train in the real-world directly---bypassing the aforementioned issues with low-fidelity simulations.
In such a context, sample-efficient learning is also paramount, as training on the real-world is inherently time-bottlenecked.

Off-policy algorithms like Soft Actor-Critic (SAC)~\citep{haarnojaSoftActorCriticOffPolicy2018} tend to be more sample efficient then their on-policy counterpart~\citep{schulmanProximalPolicyOptimization2017}, due to the presence a \emph{replay buffer} used over the course of the training.
Other than allowing to re-use transitions \( \sars \) over the course of training, the replay buffer can also accomodate for the injection of previously-collected data in the training process~\citep{ballEfficientOnlineReinforcement2023}.
Using expert demonstrations to guide learning together with learned rewards, RL training can effectively be carried out in the real-world~\citep{luoSERLSoftwareSuite2025}.
Interestingly, when completed with in-training human interventions, real-world RL agents have been shown to learn policies with near-perfect success rates on challenging manipulation tasks in 1-2 hours~\citep{luoPreciseDexterousRobotic2024}.

% DQN to DDPG to SAC
\paragraph{Sample-efficient RL}
In an MDP, the optimal policy \( \pi^* \) can be derived from its associated \qfunction, \( Q_{\pi^*} \), and in particular the optimal action(s) \(\mu(\state)\) can be selected maximizing the optimal \qfunction \ over the action space,
\[
\mu(\state) = \max_{\action \in \mathcal A} Q_{\pi^*}(\state, \action).
\]
Interestingly, the \qopt-function satisfies a recursive relationship (\emph{Bellman equation}) based on a very natural intuition%
\footnote{Quote from~\citet{mnihPlayingAtariDeep2013}. The notation used has slightly been adapted for consistency with the rest of this tutorial.}:
\begin{quote}
    [...] If the optimal value \( Q^*(\stateplusone, a_{t+1}) \) of the [state] \(\stateplusone \) was known for all possible actions \(a_{t+1} \), then the optimal strategy is to select the action \( a_{t+1}\) maximizing the expected value of \( r_t + \gamma Q^*(s_{t+1}, a_{t+1}) \)
\[ 
Q^*(s_t, a_t) = \mathbb E_{s_{t+1} \sim \mathbb P(\bullet \vert s_t, a_t)} \left[ r_t + \gamma \max_{a_{t+1} \in \mathcal A} Q^*(s_{t+1}, a_{t+1}) \big\vert s_t, a_t  \right]
\]
\end{quote}

In turn, the optimal \qfunction \ %
is guaranteed to be self-consistent by definition.
\emph{Value-iteration} methods exploit this relationship (and/or its state-value counterpart, \( V^*(s_t) \) ) by iteratively updating an initial estimate of \qopt, \( Q_k \) using the Bellman equation as update rule (\emph{Q-learning}):
\[
    Q_{i+1}(s_t, a_t) \leftarrow \mathbb E_{s_{t+1} \sim \mathbb P(\bullet \vert s_t, a_t)} \left[ r_t + \gamma \max_{a_{t+1} \in \mathcal A} Q_i (s_{t+1}, a_{t+1}) \big\vert s_t, a_t  \right],  \quad i=0,1,2,\dots,K
\]
Then, one can derive the (ideally, near-optimal) policy by explicitly maximizing over the action space the final (ideally, near-optimal) estimate \( Q_K \approx Q^* \) at each timestep. 
In fact, under certain assumptions on the MDP considered, \( Q_K \to Q^* \, \text{as } K \to \infty \).

Effective in its early applications to small-scale discrete problems and theoretically sound, vanilla Q-learning was found complicated to scale to large \( \statespace \times \actionspace \) problems, in which the storing of \( Q : \statespace \times \actionspace \mapsto \mathbb R \) alone might result prohibitive. 
Also, vanilla Q-learning is not directly usable for \emph{continuous}, unstructured state-action space MPDs, such as those considered in robotics.
In their seminal work on \emph{Deep Q-Learning} (DQN),~\citet{mnihPlayingAtariDeep2013} propose learning Q-values using deep convolutional neural networks, thereby accomodating for large and even unstructured \emph{state} spaces.
DQN parametrizes the Q-function using a neural network with parameters \( \theta \), updating the parameters by sequentially minimizing the expected squared temporal-difference error (TD-error, \( \delta_i \)):
\begin{align}
\mathcal L(\theta_i) &= \mathbb E_{(s_t, a_t) \sim \chi(\bullet)} 
    \big[ 
        (\underbrace{y_i - Q_{\theta_i}(s_t, a_t)}_{\delta_i})^2 
    \big], \label{eq:dqn-loss} \\
    y_i &= \mathbb E_{s_{t+1} \sim \mathbb P(\bullet \vert s_t, a_t)} \big[ r_t + \gamma \max_{\action \in \mathcal A} Q_{\theta_{i-1}} (\stateplusone, a_{t+1}) \big], \label{eq:TD-target}
\end{align}
Where \( \chi \) represents a behavior distribution over state-action pairs. 
Crucially, \( \chi \) can in principle be different from the policy being followed, effectively allowing to reuse prior data stored in a \emph{replay buffer} in the form of \( \sars \) transitions, used to form the TD-target \( y_i \), TD-error \( \delta_i \) and loss function~\ref{eq:dqn-loss} via Monte-Carlo (MC) estimates.

While effective in handling large, unstructured state spaces for discrete action-space problems, DQN application's to continous control problems proved challenging.
Indeed, in the case of high-capacity function approximators such as neural networks, solving \( \max_{a_t \in \mathcal A} Q_\theta(s_t, a_t) \) at each timestep is simply unfeasible due to the (1) continous nature of the action space (\( \actionspace \subset \mathbb R^n \) for some \( n \)) and (2) impossibility to express the find a cheap (ideally, closed-form) solution to \( Q_\theta \).
~\citet{silverDeterministicPolicyGradient2014} tackle this fundamental challenge by using a \emph{deterministic} function of the state \( s_t \) as policy, \( \mu_\phi(s_t) = a_t \), parametrized by \( \phi \). Thus, policies can be iteratively refined updating \( \phi \) along the direction:
\begin{equation}\label{eq:deterministic-pg}
    d_\phi = \mathbb E_{s_t \sim \mathbb P (\bullet)} \left[ \nabla_\phi Q(s_t, a_t)\vert_{a_t = \mu_\phi(s_t)} \right] = \mathbb E_{s_t \sim \mathbb P(\bullet)} \left[ \nabla_{a_t} Q(s_t, a_t) \vert_{a_t = \mu_\phi(s_t)} \cdot \nabla_\phi \mu(s_t) \right]
\end{equation}
Provably, \ref{eq:deterministic-pg} is the \emph{deterministic policy gradient} (DPG) of the policy \(\mu_\phi \)~\citep{silverDeterministicPolicyGradient2014}, so that updates \( \phi_{k+1}\leftarrow \phi_k + \alpha d_\phi \) are guaranteed to increase the (deterministic) cumulative discounted reward, \( J(\mu_\phi) \).
~\citet{lillicrapContinuousControlDeep2019} extended DPG to the case of (1) high-dimensional unstructured observations and (2) continuous action spaces, introducing Deep Deterministic Policy Gradient (DDPG), an important algorithm RL and its applications to robotics.
DDPG adopts a modified TD-target compared to the one defined in~\ref{eq:TD-target}, by maintaining a policy network used to select actions, yielding
\begin{equation}\label{eq:TD-target-ddpg}
y_i = \mathbb E_{s_{t+1} \sim \mathbb P(\bullet \vert s_t, a_t)} \big[ r_t + \gamma Q_{\theta_{i-1}} (\stateplusone, \mu_\phi(\stateplusone)) \big] .
\end{equation}
Similarily to DQN, DDPG also employs the same replay buffer mechanism, to reuse past transitions over training for increased sample efficiency and estimate the loss function via MC-estimates.

Soft Actor-Critic (SAC)~\citep{haarnojaSoftActorCriticOffPolicy2018} is a derivation of DDPG in the max-entropy (MaxEnt) RL framework, in which RL agents are tasked with \highlight{maximizing the discounted cumulative reward, while acting as randomly as possible}.
MaxEnt RL~\citep{haarnojaReinforcementLearningDeep2017} has proven particularly robust thanks to the development of diverse behaviors, incentivized by its entropy-regularization formulation.
In that, MaxEnt revisits the RL objective \( J (\pi) \) to specifically account for the policy entropy,
\begin{align}
    J(\pi) &= \sum_{t=0}^T \mathbb{E}_{(s_t, a_t) \sim \chi} \left[ r_t + \alpha \mathcal H(\pi (\bullet \vert s_t)) \right] \label{eq:J-soft}
\end{align}
This modified objective results in the \emph{soft} TD-target:
\begin{equation}\label{eq:soft-td-target}
    y_i = \mathbb E_{s_{t+1} \sim \mathbb P( \bullet \vert s_t, a_t)} \left[ r_t + \gamma \left( Q_{\theta_{i-1}} (\stateplusone, a_{t+1}) - \alpha \log \pi_\phi(a_{t+1} \vert \stateplusone) \right) \right], \quad a_{t+1} \sim \pi_\phi(\bullet \vert s_t)
\end{equation}
Similarily to DDPG, SAC also maintains an explicit policy, trained under the same MaxEnt framework for the maximization of \ref{eq:J-soft}, and updated using:
\begin{equation}\label{eq:sac-policy-update}
    \pi_{k+1} \leftarrow \arg\min_{\pi^\prime \in \Pi} \DKL \left(\pi^\prime (\bullet \vert \state) \bigg\Vert \frac{\exp(Q_{\pi_k}(s_t, \bullet))}{Z_{\pi_k}(s_t)} \right)
\end{equation}
The update rule provided in \ref{eq:sac-policy-update} optimizes the policy while projecting it on a set \( \Pi \) of tractable distributions (e.g., Gaussians,~\citet{haarnojaReinforcementLearningDeep2017}).

% SAC + prior data: RLPD
\paragraph{Sample-efficient, data-driven RL}
Importantly, sampling \( \sars \) from the replay buffer \( D \) conveniently allows to approximate the previously introduced expectations for TD-target and TD-error through Monte-Carlo (MC) estimates.
The replay buffer \( D \) also proves extremely useful in maintaining a history of previous transitions and using it for training, improving on sample efficiency.
Furthermore, it also naturally provides an entry point to inject offline trajectories recorded, for instance, by a human demonstrator, into the training process.

Reinforcement Learning with Prior Data (RLPD)~\citep{ballEfficientOnlineReinforcement2023} is an Offline-to-Online RL algorithm leveraging prior data to effectively accelerate the training of a SAC agent.
Unlike previous works on Offline-to-Online RL, RLPD avoids any pre-training and instead uses the available offline data \( D_\text{offline} \) to improve online-learning from scratch.
During each training step, transitions from both the offline and online replay buffers are sampled in equal proportion, and used in the underlying SAC routine.

% RLPD + reward classifier: SERL
\paragraph{Sample-efficient, data-driven, real-world RL}
Despite the possibility to leverage offline data for learning, the effectiveness of real-world RL training is still limited by the need to define a task-specific, hard-to-define reward function.
Further, even assuming to have access to a well-defined reward function, typical robotics pipelines rely mostly on propioperceptive inputs augmented by camera streams of the environment.
As such, even well-defined rewards would need to be derived from processed representations of unstructured observations, introducing brittleness.
In their technical report,~\citet{luoSERLSoftwareSuite2025} empirically address the needs (1) to define a reward function and (2) to use it on image observations, by introducing a series of tools to allow for streamlined training of \emph{reward classifiers} \( c \), as well as jointly learn forward-backward controllers to speed up real-world RL.
Reward classifiers are particularly useful in treating complex tasks---e.g., folding a t-shirt---for which a precise reward formulation is arbitrarily complex to obtain, or that do require significant shaping and are more easily learned directly from demonstrations of success (\(e^+\)) or failure (\(e^-\)) states, \( s \in \statespace \), with a natural choice for the state-conditioned reward function being \( r \mathcal S \mapsto \mathbb R \) being \( r(s) = \log c(e^+ \ vert s ) \).
Further,~\citet{luoSERLSoftwareSuite2025} demonstrate the benefits of learning \emph{forward} (executing the task from initial state to completion) and \emph{backward} (resetting the environment to the initial state from completion) controllers, parametrized by separate policies.

Lastly, in order to improve on the robustness of their approach to different goals while maintaing practical scalability,~\citet{luoSERLSoftwareSuite2025} introduced a modified state and action space, expressing proprioperceptive configurations \( q \)  and actions \( \dot q \) in the frame of end-effector pose at \( t=0 \).
Randomizing the initial pose of the end-effector (\( s_0 \)),\citet{luoSERLSoftwareSuite2025} achieved a similar result to that of having to manually randomize the environment at every timestep, but with the benefit of maintaining the environment in the same condition across multiple training episodes, achieving higher scalability of their method thanks to the increased practicality of their approach.

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{figures/ch3/ch3-hil-serl-examples.png}
    \caption{(A) HIL-SERL allows for real-world training of high performance RL agents by building on top advancements presented by of SAC, RLPD and SERL. (B) Example of human intervention during a HIL-SERL training process on a SO-100.}
    \label{fig:hil-serl-blocks}
\end{figure}

% SERL + Human in the loop: HIL-SERL
Building on off-policy deep Q-learning with replay buffers, entropy regularization for better exploration and performance, expert demonstrations to guide learning, and a series of tools and recommendations for real-world training using reward classifiers (Figure~\ref{fig:hil-serl-blocks}),~\citet{luoPreciseDexterousRobotic2024} introduce human interactions during training, learning near-optimal policies in challenging real-world manipulation tasks in 1-2 hours.

Human in the Loop Sample Efficient Robot reinforcement Learning (HIL-SERL)~\citep{luoPreciseDexterousRobotic2024} augments offline-to-online RL with targeted human corrections during training, and employs prior data to (1) train a reward classifier and (2) bootstrap RL training on expert trajectories.
While demonstrations provide the initial dataset seeding learning and constraining early exploration, interactive corrections allow a human supervisor to intervene on failure modes and supply targeted interventions to aid the learning process.
Crucially, human interventions are stored in both the offline and online replay buffers, differently from the autonomous transitions generated at training time and stored in the online buffer only.
Consequently, given an intervention timestep \( k \in (0, T) \), length-\(K\) human intervention data \( \{ s^{\text{human}}_k, a^{\text{human}}_k, r^{\text{human}}_k, s^{\text{human}}_{k+1},\}_{k=1}^K \) is more likely to be sampled for off-policy learning than the data generated online during training, providing stronger supervision to the agent while still allowing for autonomous learning.
Empirically, HIL-SERL attains near-perfect success rates on diverse manipulation tasks within 1-2 hours of training~\citep{luoPreciseDexterousRobotic2024}, underscoring how offline datasets with online RL can markedly improve stability and data efficiency, and ultimately even allow real-world RL-training.

\subsubsection{Code Example: Real-world RL}
\textbf{TODO(fracapuano): work out rl training example}

\subsubsection{Limitations of RL in Real-World Robotics: Simulators and Reward Design}

Despite the advancements in real-world RL training, solving robotics training RL agents in the real world still suffers from the following limitations:
\begin{itemize}
\item In those instances where real-world training experience is prohibitively expensive to gather~\citep{degraveMagneticControlTokamak2022, bellemareAutonomousNavigationStratospheric2020}, in-simulation training is often the only option. However, high-fidelity simulators for real-world problems can be difficult to build and maintain, especially for contact-rich manipulation and tasks involving deformable or soft materials.

\item Reward design poses an additional source of brittleness. Dense shaping terms are often required to guide exploration in long-horizon problems, but poorly tuned terms can lead to specification gaming or local optima. Sparse rewards avoid shaping but exacerbate credit assignment and slow down learning. In practice, complex behaviors require efforts shaping rewards: a britlle and error prone process.
\end{itemize}

Advances in Behavioral Cloning (BC) from corpora of human demonstrations address both of these concerns.
By learning in a supervised fashion to reproduce expert demonstrations, BC methods prove competitive while bypassing the need for simulated environments and hard-to-define reward functions.
