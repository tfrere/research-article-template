<div class="d3-tokenization-comparison"></div>

<style>
  .d3-tokenization-comparison {
    font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
    line-height: 1.5;
    color: var(--text-color);
  }
  
  .d3-tokenization-comparison .comparison-grid {
    display: grid;
    grid-template-columns: 1fr 1fr;
    gap: 20px;
    margin-bottom: 20px;
  }
  
  .d3-tokenization-comparison .tokenizer-section {
    background: var(--page-bg);
    border: 1px solid var(--border-color);
    border-radius: 8px;
    padding: 20px;
    position: relative;
  }
  
  .d3-tokenization-comparison .gpt2-section {
    border: 2px solid #e74c3c;
  }
  
  .d3-tokenization-comparison .gemma-section {
    border: 2px solid #27ae60;
  }
  
  
  .d3-tokenization-comparison .tokenizer-title {
    font-size: 1.1em;
    font-weight: 700;
    margin-bottom: 20px;
    text-align: center;
    text-transform: uppercase;
    letter-spacing: 0.5px;
  }
  
  .d3-tokenization-comparison .gpt2-title {
    color: #e74c3c;
  }
  
  .d3-tokenization-comparison .gemma-title {
    color: #27ae60;
  }
  
  .d3-tokenization-comparison .language-section {
    margin-bottom: 20px;
  }
  
  .d3-tokenization-comparison .language-name {
    font-size: 0.9em;
    font-weight: 600;
    margin-bottom: 8px;
    color: var(--text-color);
    text-transform: uppercase;
    letter-spacing: 0.3px;
  }
  
  .d3-tokenization-comparison .sentence-text {
    font-size: 0.85em;
    margin-bottom: 12px;
    padding: 12px;
    background: var(--surface-bg);
    border-radius: 6px;
    border: 1px solid var(--border-color);
    line-height: 1.5;
  }
  
  .d3-tokenization-comparison .arabic-text {
    direction: rtl;
    text-align: right;
    font-family: 'Noto Sans Arabic', 'Arial Unicode MS', sans-serif;
  }
  
  .d3-tokenization-comparison .token-count {
    font-size: 0.85em;
    color: var(--primary-color);
    margin-bottom: 10px;
    font-weight: 700;
    text-align: center;
    background: var(--surface-bg);
    border: 1px solid var(--border-color);
    border-radius: 6px;
    padding: 6px 12px;
    display: inline-block;
    box-shadow: 0 1px 3px rgba(0,0,0,0.1);
  }
  
  .d3-tokenization-comparison .token-grid {
    display: flex;
    flex-wrap: wrap;
    gap: 3px;
    margin-bottom: 12px;
    justify-content: flex-start;
  }
  
  .d3-tokenization-comparison .token {
    padding: 4px 8px;
    border-radius: 4px;
    font-size: 0.75em;
    font-weight: 500;
    color: white;
    cursor: pointer;
    transition: transform 0.2s;
    border: 1px solid rgba(255,255,255,0.2);
    min-width: 16px;
    text-align: center;
    position: relative;
  }
  
  .d3-tokenization-comparison .token:hover {
    transform: scale(1.05);
    z-index: 10;
  }
  
  .d3-tokenization-comparison .token-id {
    font-size: 0.7em;
    color: rgba(255,255,255,0.8);
    margin-top: 1px;
    line-height: 1;
  }
  
  .d3-tokenization-comparison .delimited-text {
    display: none;
  }
  
  

  /* Responsive design */
  @media (max-width: 768px) {
    .d3-tokenization-comparison .comparison-grid {
      grid-template-columns: 1fr;
      gap: 16px;
    }
    
    
    .d3-tokenization-comparison .tokenizer-section {
      padding: 16px;
    }
  }

  @media (max-width: 480px) {
    .d3-tokenization-comparison {
      padding: 12px;
    }
    
    .d3-tokenization-comparison .tokenizer-section {
      padding: 12px;
    }
    
    
  }
</style>

<script>
  (() => {
    const bootstrap = () => {
      const scriptEl = document.currentScript;
      let container = scriptEl ? scriptEl.previousElementSibling : null;
      if (!(container && container.classList && container.classList.contains('d3-tokenization-comparison'))) {
        const candidates = Array.from(document.querySelectorAll('.d3-tokenization-comparison'))
          .filter((el) => !(el.dataset && el.dataset.mounted === 'true'));
        container = candidates[candidates.length - 1] || null;
      }
      if (!container) return;
      if (container.dataset) {
        if (container.dataset.mounted === 'true') return;
        container.dataset.mounted = 'true';
      }

      // Sample data
      const sampleData = {
        english_text: "The development of large language models has revolutionized artificial intelligence research and applications across multiple domains especially education.",
        arabic_text: "لقد أحدث تطوير النماذج اللغوية الكبيرة ثورة في مجال أبحاث الذكاء الاصطناعي وتطبيقاته في مجالات متعددة، وخاصة في مجال التعليم.",
        gpt2: {
          english: {
            tokens: ["The", "development", "of", "large", "language", "models", "has", "revolution", "ized", "artificial", "intelligence", "research", "and", "applications", "across", "multiple", "domains", "especially", "education", "."],
            token_ids: [464, 2478, 286, 1588, 3303, 4981, 468, 5854, 1143, 11666, 4430, 2267, 290, 5479, 1973, 3294, 18209, 2592, 3707, 13],
            token_count: 20,
            delimited_text: "The|development|of|large|language|models|has|revolution|ized|artificial|intelligence|research|and|applications|across|multiple|domains|especially|education|."
          },
          arabic: {
            tokens: ["ÙĦ", "Ù", "Ĥ", "Ø¯", "Ø", "£", "Ø", "Ń", "Ø¯", "Ø", "«", "Ø", "ª", "Ø", "·", "ÙĪ", "ÙĬ", "Ø±", "Ø§ÙĦ", "ÙĨ", "Ùħ", "Ø§Ø", "°", "Ø", "¬", "Ø§ÙĦ", "ÙĦ", "Ø", "º", "ÙĪ", "ÙĬ", "Ø©", "Ø§ÙĦ", "Ù", "ĥ", "Ø¨", "ÙĬ", "Ø±", "Ø©", "Ø", "«", "ÙĪ", "Ø±", "Ø©", "Ù", "ģ", "ÙĬ", "Ùħ", "Ø", "¬", "Ø§ÙĦ", "Ø", "£", "Ø¨", "Ø", "Ń", "Ø§Ø", "«", "Ø§ÙĦ", "Ø", "°", "Ù", "ĥ", "Ø§Ø", "¡", "Ø§ÙĦ", "Ø§Ø", "µ", "Ø", "·", "ÙĨ", "Ø§Ø", "¹", "ÙĬ", "ÙĪ", "Øª", "Ø", "·", "Ø¨", "ÙĬ", "Ù", "Ĥ", "Ø§Ø", "ª", "Ùĩ", "Ù", "ģ", "ÙĬ", "Ùħ", "Ø", "¬", "Ø§ÙĦ", "Ø§Ø", "ª", "Ùħ", "Øª", "Ø¹", "Ø¯", "Ø¯", "Ø©", "Ø", "Į", "ÙĪ", "Ø", "®", "Ø§Ø", "µ", "Ø©", "Ù", "ģ", "ÙĬ", "Ùħ", "Ø", "¬", "Ø§ÙĦ", "Ø§ÙĦ", "Øª", "Ø¹", "ÙĦ", "ÙĬ", "Ùħ", "."],
            token_ids: [13862, 149, 224, 38843, 17550, 96, 148, 255, 38843, 148, 104, 17550, 103, 148, 115, 30335, 22654, 26897, 28981, 23338, 25405, 34247, 108, 148, 105, 28981, 13862, 148, 118, 30335, 22654, 45632, 28981, 149, 225, 39848, 22654, 26897, 45632, 17550, 104, 30335, 26897, 45632, 18923, 223, 22654, 47048, 148, 105, 23525, 17550, 96, 39848, 148, 255, 34247, 104, 28981, 148, 108, 149, 225, 34247, 94, 28981, 34247, 113, 148, 115, 23338, 34247, 117, 22654, 42092, 41486, 148, 115, 39848, 22654, 149, 224, 34247, 103, 29519, 18923, 223, 22654, 47048, 148, 105, 23525, 34247, 103, 47048, 41486, 44690, 38843, 38843, 45632, 148, 234, 42092, 148, 106, 34247, 113, 45632, 18923, 223, 22654, 47048, 148, 105, 23525, 28981, 41486, 44690, 13862, 22654, 25405, 13],
            token_count: 122,
            delimited_text: "ÙĦ|Ù|Ĥ|Ø¯|Ø|£|Ø|Ń|Ø¯|Ø|«|Ø|ª|Ø|·|ÙĪ|ÙĬ|Ø±|Ø§ÙĦ|ÙĨ|Ùħ|Ø§Ø|°|Ø|¬|Ø§ÙĦ|ÙĦ|Ø|º|ÙĪ|ÙĬ|Ø©|Ø§ÙĦ|Ù|ĥ|Ø¨|ÙĬ|Ø±|Ø©|Ø|«|ÙĪ|Ø±|Ø©|Ù|ģ|ÙĬ|Ùħ|Ø|¬|Ø§ÙĦ|Ø|£|Ø¨|Ø|Ń|Ø§Ø|«|Ø§ÙĦ|Ø|°|Ù|ĥ|Ø§Ø|¡|Ø§ÙĦ|Ø§Ø|µ|Ø|·|ÙĨ|Ø§Ø|¹|ÙĬ|ÙĪ|Øª|Ø|·|Ø¨|ÙĬ|Ù|Ĥ|Ø§Ø|ª|Ùĩ|Ù|ģ|ÙĬ|Ùħ|Ø|¬|Ø§ÙĦ|Ø§Ø|ª|Ùħ|Øª|Ø¹|Ø¯|Ø¯|Ø©|Ø|Į|ÙĪ|Ø|®|Ø§Ø|µ|Ø©|Ù|ģ|ÙĬ|Ùħ|Ø|¬|Ø§ÙĦ|Ø§ÙĦ|Øª|Ø¹|ÙĦ|ÙĬ|Ùħ|."
          }
        },
        gemma: {
          english: {
            tokens: ["The", "development", "of", "large", "language", "models", "has", "revolutionized", "artificial", "intelligence", "research", "and", "applications", "across", "multiple", "domains", "especially", "education", "."],
            token_ids: [818, 2913, 529, 2455, 5192, 4681, 815, 176839, 16477, 14020, 2958, 532, 6182, 3418, 5065, 26104, 4285, 4993, 236761],
            token_count: 19,
            delimited_text: "The|development|of|large|language|models|has|revolutionized|artificial|intelligence|research|and|applications|across|multiple|domains|especially|education|."
          },
          arabic: {
            tokens: ["لق", "د", "أ", "حدث", "تطوير", "الن", "ما", "ذج", "الل", "غ", "وية", "الك", "بيرة", "ث", "ورة", "في", "مجال", "أ", "بح", "اث", "الذ", "ك", "اء", "الاص", "ط", "نا", "عي", "وت", "ط", "بيقات", "ه", "في", "مج", "الات", "متعدد", "ة", "،", "وخ", "اص", "ة", "في", "مجال", "التعليم", "."],
            token_ids: [16659, 236897, 3506, 53963, 225742, 9602, 4697, 146343, 4753, 237334, 63704, 11420, 83373, 13292, 37930, 3170, 124198, 3506, 54694, 132498, 19337, 237005, 9751, 176504, 237076, 2932, 33210, 19065, 237076, 127065, 236910, 3170, 18263, 47481, 138367, 237049, 237108, 78471, 17009, 237049, 3170, 124198, 70948, 236761],
            token_count: 44,
            delimited_text: "لق|د|أ|حدث|تطوير|الن|ما|ذج|الل|غ|وية|الك|بيرة|ث|ورة|في|مجال|أ|بح|اث|الذ|ك|اء|الاص|ط|نا|عي|وت|ط|بيقات|ه|في|مج|الات|متعدد|ة|،|وخ|اص|ة|في|مجال|التعليم|."
          }
        }
      };

      // Create the interface
      container.innerHTML = `
        <div class="comparison-grid">
          <div class="tokenizer-section gpt2-section">
            <div class="tokenizer-title gpt2-title">GPT-2</div>
            
            <div class="language-section">
              <div class="language-name">English</div>
              <div class="sentence-text" id="gpt2EnglishText">${sampleData.english_text}</div>
              <div class="token-count" id="gpt2EnglishTokenCount">20 tokens</div>
              <div class="token-grid" id="gpt2EnglishTokens"></div>
              <div class="delimited-text" id="gpt2EnglishDelimited">${sampleData.gpt2.english.delimited_text}</div>
            </div>
            
            <div class="language-section">
              <div class="language-name">Arabic</div>
              <div class="sentence-text arabic-text" id="gpt2ArabicText">${sampleData.arabic_text}</div>
              <div class="token-count" id="gpt2ArabicTokenCount">122 tokens</div>
              <div class="token-grid" id="gpt2ArabicTokens"></div>
              <div class="delimited-text arabic-text" id="gpt2ArabicDelimited">${sampleData.gpt2.arabic.delimited_text}</div>
            </div>
          </div>
          
          <div class="tokenizer-section gemma-section">
            <div class="tokenizer-title gemma-title">Gemma</div>
            
            <div class="language-section">
              <div class="language-name">English</div>
              <div class="sentence-text" id="gemmaEnglishText">${sampleData.english_text}</div>
              <div class="token-count" id="gemmaEnglishTokenCount">19 tokens</div>
              <div class="token-grid" id="gemmaEnglishTokens"></div>
              <div class="delimited-text" id="gemmaEnglishDelimited">${sampleData.gemma.english.delimited_text}</div>
            </div>
            
            <div class="language-section">
              <div class="language-name">Arabic</div>
              <div class="sentence-text arabic-text" id="gemmaArabicText">${sampleData.arabic_text}</div>
              <div class="token-count" id="gemmaArabicTokenCount">44 tokens</div>
              <div class="token-grid" id="gemmaArabicTokens"></div>
              <div class="delimited-text arabic-text" id="gemmaArabicDelimited">${sampleData.gemma.arabic.delimited_text}</div>
            </div>
          </div>
        </div>
      `;

      // Cache for randomized palette
      let randomizedColors = null;
      
      function getRandomizedColors() {
        if (!randomizedColors) {
          const baseColors = window.ColorPalettes ? window.ColorPalettes.getColors('categorical', 20) : [
            '#ff6b6b', '#4ecdc4', '#45b7d1', '#96ceb4', '#feca57',
            '#ff9ff3', '#54a0ff', '#5f27cd', '#00d2d3', '#ff9f43',
            '#10ac84', '#ee5a24', '#0984e3', '#6c5ce7', '#a29bfe',
            '#fd79a8', '#fdcb6e', '#e17055', '#74b9ff', '#00b894'
          ];
          
          // Fisher-Yates shuffle algorithm
          randomizedColors = [...baseColors];
          for (let i = randomizedColors.length - 1; i > 0; i--) {
            const j = Math.floor(Math.random() * (i + 1));
            [randomizedColors[i], randomizedColors[j]] = [randomizedColors[j], randomizedColors[i]];
          }
        }
        return randomizedColors;
      }
      
      function getTokenColor(index) {
        const colors = getRandomizedColors();
        return colors[index % colors.length];
      }
      
      function hasArabicChars(token) {
        const arabicChars = 'ابتثجحخدذرزسشصضطظعغفقكلمنهوي';
        return [...token].some(char => arabicChars.includes(char));
      }
      
      function generateTokensHTML(tokens, tokenIds, language) {
        return tokens.map((token, index) => {
          const color = getTokenColor(index);
          const isArabic = hasArabicChars(token);
          const borderStyle = isArabic ? '2px solid #27ae60' : '1px solid rgba(255,255,255,0.2)';
          return `
            <div class="token" style="background-color: ${color}; border: ${borderStyle};" 
                 title="Token: ${token} | ID: ${tokenIds[index]}">
              ${token}
              <div class="token-id">${tokenIds[index]}</div>
            </div>
          `;
        }).join('');
      }
      
      function updateVisualization(data) {
        // Update texts
        container.querySelector('#gpt2EnglishText').textContent = data.english_text;
        container.querySelector('#gpt2ArabicText').textContent = data.arabic_text;
        container.querySelector('#gemmaEnglishText').textContent = data.english_text;
        container.querySelector('#gemmaArabicText').textContent = data.arabic_text;
        
        // Update token counts
        container.querySelector('#gpt2EnglishTokenCount').textContent = `${data.gpt2.english.token_count} tokens`;
        container.querySelector('#gpt2ArabicTokenCount').textContent = `${data.gpt2.arabic.token_count} tokens`;
        container.querySelector('#gemmaEnglishTokenCount').textContent = `${data.gemma.english.token_count} tokens`;
        container.querySelector('#gemmaArabicTokenCount').textContent = `${data.gemma.arabic.token_count} tokens`;
        
        // Update token grids
        container.querySelector('#gpt2EnglishTokens').innerHTML = generateTokensHTML(data.gpt2.english.tokens, data.gpt2.english.token_ids, 'english');
        container.querySelector('#gpt2ArabicTokens').innerHTML = generateTokensHTML(data.gpt2.arabic.tokens, data.gpt2.arabic.token_ids, 'arabic');
        container.querySelector('#gemmaEnglishTokens').innerHTML = generateTokensHTML(data.gemma.english.tokens, data.gemma.english.token_ids, 'english');
        container.querySelector('#gemmaArabicTokens').innerHTML = generateTokensHTML(data.gemma.arabic.tokens, data.gemma.arabic.token_ids, 'arabic');
        
        // Update delimited texts
        container.querySelector('#gpt2EnglishDelimited').textContent = data.gpt2.english.delimited_text;
        container.querySelector('#gpt2ArabicDelimited').textContent = data.gpt2.arabic.delimited_text;
        container.querySelector('#gemmaEnglishDelimited').textContent = data.gemma.english.delimited_text;
        container.querySelector('#gemmaArabicDelimited').textContent = data.gemma.arabic.delimited_text;
        
      }
      
      // Initialize with sample data
      updateVisualization(sampleData);
    };

    // Load color palettes script if not already loaded
    if (!window.ColorPalettes) {
      const script = document.createElement('script');
      script.src = '/scripts/color-palettes.js';
      script.onload = () => {
        // Re-render with proper colors after palette script loads
        if (container && container.querySelector) {
          updateVisualization(sampleData);
        }
      };
      document.head.appendChild(script);
    }

    if (document.readyState === 'loading') {
      document.addEventListener('DOMContentLoaded', bootstrap, { once: true });
    } else {
      bootstrap();
    }
  })();
</script>
